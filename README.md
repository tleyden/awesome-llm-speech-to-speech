
üë• Peer reviewed on [this r/LocalLlama thread](https://www.reddit.com/r/LocalLLaMA/comments/1nxqabe/awesome_local_llm_speechtospeech_models_frameworks/) 

‚≠ê If this is a useful resource, [star the repo on GitHub](https://github.com/vtleyden/awesome-llm-speech-to-speech/stargazers).

# Awesome LLM Speech-to-Speech models and frameworks

Everything in the list:

* Integrates with an LLM.  Either as a separate module, or has the LLM baked-in as part of a speech-to-speech model 
* Supports speech-to-speech, not just speech-to-text or text-to-speech

If you know of any corrections or missing entries, help curate the list by [opening a PR](https://github.com/tleyden/awesome-llm-speech-to-speech/pulls).

### Local 

* Libraries, frameworks, or models that can be run locally

| Project                                                                                                              | Open Source             | e2e or cascading model                                                          | Supported LLMs                                | Supports tool calling                 | Supported Platforms | Requires GPU + CUDA         | Supports barge-in | Demo                                                         | Supports voice cloning                | Supported languages        | Cost                                          | Includes API Server                                                                          | Additional Cmments                    |
| :----------------------------------------------------------------------------------------------------------------------- | :---------------------- | ------------------------------------------------------------------------------- | --------------------------------------------- | ------------------------------------- | --------------------------------- | --------------------------- | ----------------- | ------------------------------------------------------------ | ------------------------------------- | -------------------------- | --------------------------------------------- | -------------------------------------------------------------------------------------------- | ------------------------------------- |
| [Unmute.sh](https://github.com/kyutai-labs/unmute)                                                                       | Yes                     | Cascading                                                                       | Any local LLM                                 | Not yet, but looking for contributors | Linux only                        | Yes                         | Yes               | [Interactive Demo](https://unmute.sh/)                       | Yes                                   | EN, FR                     | Free                                          | Yes, mostly compatible with OpenAI Realtime Speech                                           | From Kyutai, makers of Moshi          |
| [Speaches](https://github.com/speaches-ai/speaches)                                                                     | Yes (MIT)               | Cascading                                                                       | Any OpenAI-compatible LLM (local or remote)   | Yes (via OpenAI-compatible API)       | Windows/Linux/Mac/Docker          | Optional (GPU recommended)       | Yes               | [Recorded Demo](https://github.com/speaches-ai/speaches)    | Yes (via Kokoro and Piper)           | EN, FR, ES, ZH, JA, KO       | Free / Self-hosted                              | Yes (OpenAI-compatible API endpoints for /v1/audio and /v1/chat)                            | ‚ÄúOllama for Speech.‚Äù  Supports realtime transcription, translation, and TTS via Docker.       |
| [Ultravox (Fixie)](https://github.com/fixie-ai/ultravox)                                                                 | Yes (open-weight / MIT) | Hybrid: audio-native LLM with ASR, separate TTS | Llama/Mistral/Gemma backbones                 | Yes, via underlying LLM<br>           | Windows/Linux                     | Yes                         | Yes               | [Interactive Demo](https://demo.ultravox.ai/)                | N/A (it‚Äôs text-out; use your TTS)<br> | EN                         | Free to use model, hosted commercial offering | Only in hosted commercial offering                                                           |                                       |
| [RealtimeVoiceChat](https://github.com/KoljaB/RealtimeVoiceChat)                                                         | Yes (MIT)               | Cascading                                                                       | Pluggable LLM, including remote APIs          | Probably via underlying LLM           | Linux recommended, Windows maybe  | Yes                         | Yes               | [Recorded Demo](https://github.com/KoljaB/RealtimeVoiceChat) | Yes, via pluggable TTS engines        | EN, what else?             | Free                                          | Web Audio API (websocket based)                                                              | Author put it on hold for now         |
| [Vocalis](https://github.com/Lex-au/Vocalis)                                                                             | Yes (Apache 2)          | Cascading                                                                       | Includes fine tuned Meta LLaMA 3 8B Instruct  | Probably via underlying LLM           | Windows/Linux/Mac                | No                          | Yes               | [Recorded Demo](https://www.youtube.com/watch?v=2slWwsHTNIA) | Yes, via pluggable TTS engines        | ?                          | Free                                          | It's a bit unclear, filed [issue with question](https://github.com/Lex-au/Vocalis/issues/12) | Runs on Apple Silicon without a GPU!  Supports whisper.cpp, llama.cpp, and TTS engine supports Apple MPS  |
| [VoiceAssistant](https://github.com/ReisCook/VoiceAssistant)                                                         | Yes (Apache 2)               | Cascading                                                                       | Llama 3.2 1B          | Probably via underlying LLM           | Linux, Windows  | Yes                         | ?               | No Demo | Yes, via Sesame TTS engine        | EN, what else?             | Free                                          | No API                                                              | Uses Sesame, which has very high quality TTS.  Real-time factor: 0.6x with NVIDIA 4070 Ti Super.        |
| [orpheus-chat-webui](https://github.com/PkmX/orpheus-chat-webui)                                             | Yes                     | Cascading                                                                       | Pluggable LLM                                 | Probably via underlying LLM           | Windows/Linux/Mac                 | ?                           | ?                 | No Demo                                                      | Yes, via pluggable Orpheus        | EN, maybe others             | Free                     | Yes, WebRTC                                                                                           | Compatible with llama.cpp    |
| [SimpleVoiceChat](https://github.com/thiswillbeyourgithub/simple_voice_chat)                                             | Yes                     | Cascading                                                                       | Pluggable LLM                                 | Probably via underlying LLM           | Windows/Linux/Mac                 | ?                           | ?                 | No Demo                                                      | Yes, via pluggable TTS engines        | EN, what else?             | Free with classic backend                     | No                                                                                           | Vibe coded, sits on top of fastrtc    |
| [LFM2-audio](https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models) | Yes (open weights)                     | E2E                                                                             | E2E model, LLM built-in                       | [Not Yet](https://www.linkedin.com/feed/update/urn:li:ugcPost:7379151558485258241?commentUrn=urn%3Ali%3Acomment%3A%28ugcPost%3A7379151558485258241%2C7379157934766727168%29&replyUrn=urn%3Ali%3Acomment%3A%28ugcPost%3A7379151558485258241%2C7379212021336170496%29&dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287379157934766727168%2Curn%3Ali%3AugcPost%3A7379151558485258241%29&dashReplyUrn=urn%3Ali%3Afsd_comment%3A%287379212021336170496%2Curn%3Ali%3AugcPost%3A7379151558485258241%29)                                   | Windows/Linux                     | Yes                         | ?                 | [Recorded Demo](https://www.youtube.com/watch?v=1eGMxkffBC8) + [Live Demo](https://playground.liquid.ai/talk) | ?                                     | EN, JA, AR, KO, ES, FR, DE | Free                                          | No                                                                                           | From Liquid AI.  Starts at 1.5B model |
| [Mini-omni2](https://github.com/gpt-omni/mini-omni2)                                                                     | Yes (MIT)               | E2E                                                                             | E2E model, LLM built-in (Qwen2)               | ?                                     | ?                                 | ?                           | ?                 | [Recorded Demo](https://github.com/gpt-omni/mini-omni2)      | ?                                     | ?                          | Free                                          | No                                                                                           |                                       |
| [Qwen3-Omni](https://github.com/QwenLM/Qwen3-Omni) | Yes (Apache 2) | E2E | E2E model, LLM built-in (Qwen) | Unknown - not mentioned | Windows/Linux | Yes | Yes | [Live demo](https://huggingface.co/spaces/Qwen/Qwen3-Omni-Demo) | ? | EN, CN, what else? | Free | Unclear. Maybe supports OpenAI realtime speech api, plugs into vLLM | 
| [MiniCPM-o 2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)                                                            | Yes                     | E2E                                                                             | E2E model, LLM built-in                       | Unknown - not mentioned               | Windows/Linux                     | Yes                         | ?                 | Demo Offline                                                 | ?                                     | EN, CN, what else?         | Free                                          | Just a model.  Relies on other tools.                                                        |                                       |
| [Pipecat](https://github.com/pipecat-ai/pipecat)                                                                         | Yes                     | Cascading                                                                       | Pluggable LLM, ASR, and TTS                   | Yes                                   | Windows/Linux/Mac/iOS/Android     | No                          | Yes               | ?                                                            | Yes via TTS engines                   | ?                          | Free unless pairing with daily.co platform    | No, but mentions it can be used with FastAPI                                                 | Unclear on how to fully self-host it  |
| [voicechat2](https://github.com/lhl/voicechat2)                                                                          | Yes                     | Cascading                                                                       | Pluggable LLM, (any openai compatible server) | Probably, via LLM                     | Linux                             | Yes                         | No                | [Recorded Demo](https://github.com/lhl/voicechat2)           | Yes via TTS engine                    | EN, what else?             | Free                                          | Via bespoke websocket                                                                        |                                       |
| [HuggingFace SpeechToSpeech](https://github.com/huggingface/speech-to-speech)                                            | Yes                     | Cascading                                                                       | Pluggable LLM (flexible)                      | Probably, via LLM                     | Mac/Windows/Linux                 | CUDA with fallback for Macs | ?                 | None                                                         | Yes via TTS engine                    | EN, FR, ES, ZH, JA, KO     | Free                                          | No                                                                                           |                                       |
| [Sauropod](https://github.com/sauropod-io/sauropod)                                                                      | Yes                     | Cascading                                                                       | ?                                             | ?                                     | ?                                 | ?                           | ?                 | ?                                                            | ?                                     | ?                          | Free                                          | OpenAI realtime speech compatible                                                            | Author put it on hold for now         |
| [GLaDOS](https://github.com/dnhkng/GLaDOS)                                                                             | Yes (MIT)               | Cascading (ASR + LLM + TTS)                                                     | Ollama, OpenAI, or any compatible local LLM   | Yes                                   | Windows/Linux/Mac/SBC (RK3588)   | Optional (CUDA/ROCm/DirectML)    | Yes               | [Demo Video](https://github.com/dnhkng/GLaDOS)             | Yes (via Kokoro voices)              | EN (multiple accents)        | Free / Self-hosted                              | Yes (OpenAI-compatible TTS API server via Docker)                                            | Real-life Portal-style AI.  Optimized for <600 ms latency, runs even on 8 GB SBCs.           |
| [On-Device-Speech-to-Speech-Conversational-AI](https://github.com/asiff00/On-Device-Speech-to-Speech-Conversational-AI) | Yes (MIT) | Cascading (VAD ‚Üí Whisper ‚Üí LLM ‚Üí TextChunker ‚Üí Kokoro TTS) | Any Ollama model (tested with Qwen2.5 0.5B), LM Studio | Probably via underlying LLM | Windows/Linux/Mac (Python) | No (CPU-only by design) | Yes | [Recorded Demo](https://youtu.be/x92FLnwf-nA) | Yes (via Kokoro 82M) | EN (Whisper tiny.en), possibly others if Whisper replaced | Free | No | Runs entirely on CPU with ~1.5‚Äì2.0s latency, aggressive text-chunker for ultra-low latency, streaming + filler-word prompting, multi-threaded queue architecture using VAD + Whisper + Kokoro. |
| [LLMRTC](https://github.com/llmrtc/llmrtc) | Yes (Apache 2.0) | Orchestrator (Both) | Agnostic (OpenAI, Ollama, etc) | Yes | Node.js / Web | No | Yes | [Website](https://www.llmrtc.org) | Via Provider | Any | Free | Yes | WebRTC infrastructure layer. Handles server-side VAD & barge-in. |

There are a few more details on [this r/LocalLlama thread](https://www.reddit.com/r/LocalLLaMA/comments/1nxqabe/awesome_local_llm_speechtospeech_models_frameworks/) 
